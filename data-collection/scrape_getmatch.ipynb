{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ofhtqu3dbV-z"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import time\n",
        "import logging\n",
        "import re\n",
        "from typing import Dict, List, Optional\n",
        "from datetime import datetime\n",
        "\n",
        "class EnhancedJobScraper:\n",
        "    def __init__(self):\n",
        "        self.base_url = \"https://getmatch.ru/vacancies\"\n",
        "        self.headers = {\n",
        "            \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
        "        }\n",
        "        logging.basicConfig(level=logging.INFO)\n",
        "        self.logger = logging.getLogger(__name__)\n",
        "\n",
        "    def parse_salary(self, salary_text: str) -> tuple:\n",
        "        \"\"\"\n",
        "        –ü–∞—Ä—Å–∏—Ç —Å—Ç—Ä–æ–∫—É —Å –∑–∞—Ä–ø–ª–∞—Ç–æ–π –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ –∏ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ\n",
        "\n",
        "        –ü—Ä–∏–º–µ—Ä—ã –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö:\n",
        "        - \"250 000 ‚Äî 300 000 ‚ÇΩ/–º–µ—Å –Ω–∞ —Ä—É–∫–∏\"\n",
        "        - \"–æ—Ç 250 000 ‚ÇΩ/–º–µ—Å –Ω–∞ —Ä—É–∫–∏\"\n",
        "        - \"–¥–æ 300 000 ‚ÇΩ/–º–µ—Å –Ω–∞ —Ä—É–∫–∏\"\n",
        "        \"\"\"\n",
        "        if not salary_text:\n",
        "            return None, None\n",
        "\n",
        "        # –£–¥–∞–ª—è–µ–º –≤—Å–µ –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã –∏ –ø—Ä–∏–≤–æ–¥–∏–º –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É\n",
        "        salary_text = salary_text.lower().strip()\n",
        "\n",
        "        # –£–¥–∞–ª—è–µ–º \"‚ÇΩ/–º–µ—Å –Ω–∞ —Ä—É–∫–∏\" –∏ –ø–æ–¥–æ–±–Ω—ã–µ –æ–∫–æ–Ω—á–∞–Ω–∏—è\n",
        "        salary_text = re.sub(r'‚ÇΩ/–º–µ—Å.*$', '', salary_text)\n",
        "\n",
        "        # –£–¥–∞–ª—è–µ–º –≤—Å–µ –ø—Ä–æ–±–µ–ª—ã –∏–∑ —á–∏—Å–µ–ª\n",
        "        salary_text = re.sub(r'\\s(?=\\d)', '', salary_text)\n",
        "\n",
        "        # –ü—ã—Ç–∞–µ–º—Å—è –Ω–∞–π—Ç–∏ –¥–≤–∞ —á–∏—Å–ª–∞ (–¥–∏–∞–ø–∞–∑–æ–Ω)\n",
        "        range_match = re.findall(r'\\d+', salary_text)\n",
        "\n",
        "        if '–æ—Ç' in salary_text and len(range_match) == 1:\n",
        "            return int(range_match[0]), None\n",
        "        elif '–¥–æ' in salary_text and len(range_match) == 1:\n",
        "            return None, int(range_match[0])\n",
        "        elif len(range_match) >= 2:\n",
        "            return int(range_match[0]), int(range_match[1])\n",
        "        elif len(range_match) == 1:\n",
        "            return int(range_match[0]), int(range_match[0])\n",
        "\n",
        "        return None, None\n",
        "\n",
        "    def get_job_description(self, job_url: str) -> Dict:\n",
        "        \"\"\"–ò–∑–≤–ª–µ–∫–∞–µ—Ç –ø–æ–ª–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –≤–∞–∫–∞–Ω—Å–∏–∏\"\"\"\n",
        "        try:\n",
        "            response = requests.get(job_url, headers=self.headers)\n",
        "            response.raise_for_status()\n",
        "            soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "            job_data = {\n",
        "                'url': job_url,\n",
        "                'title': None,\n",
        "                'company_name': None,\n",
        "                'salary_text': None,\n",
        "                'salary_from': None,\n",
        "                'salary_to': None,\n",
        "                'location': None,\n",
        "                'work_format': None,\n",
        "                'specialization': None,\n",
        "                'level': None,\n",
        "                'company_logo_url': None,\n",
        "                'description_text': [],\n",
        "                'skills': [],\n",
        "                'posted_date': None\n",
        "            }\n",
        "\n",
        "            # –ó–∞–≥–æ–ª–æ–≤–æ–∫ –≤–∞–∫–∞–Ω—Å–∏–∏\n",
        "            title_elem = soup.find('h1')\n",
        "            if title_elem:\n",
        "                job_data['title'] = title_elem.text.strip()\n",
        "\n",
        "            # –ù–∞–∑–≤–∞–Ω–∏–µ –∫–æ–º–ø–∞–Ω–∏–∏\n",
        "            company_elem = soup.find('h2')\n",
        "            if company_elem and company_elem.find('a'):\n",
        "                job_data['company_name'] = company_elem.find('a').text.strip()\n",
        "\n",
        "            # –ó–∞—Ä–ø–ª–∞—Ç–∞\n",
        "            salary_elem = soup.find('h3')\n",
        "            if salary_elem:\n",
        "                salary_text = salary_elem.text.strip()\n",
        "                job_data['salary_text'] = salary_text\n",
        "                job_data['salary_from'], job_data['salary_to'] = self.parse_salary(salary_text)\n",
        "\n",
        "            # –õ–æ–∫–∞—Ü–∏—è –∏ —Ñ–æ—Ä–º–∞—Ç —Ä–∞–±–æ—Ç—ã\n",
        "            location_container = soup.find('div', class_='b-vacancy-locations')\n",
        "            if location_container:\n",
        "                locations = location_container.find_all('span', class_='g-label')\n",
        "                for loc in locations:\n",
        "                    if 'üìç' in loc.text:\n",
        "                        job_data['location'] = loc.text.replace('üìç', '').strip()\n",
        "                    else:\n",
        "                        job_data['work_format'] = loc.text.strip()\n",
        "\n",
        "            # –°–ø–µ—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∏ —É—Ä–æ–≤–µ–Ω—å\n",
        "            specs_container = soup.find('div', class_='b-specs')\n",
        "            if specs_container:\n",
        "                rows = specs_container.find_all('div', class_='row')\n",
        "                for row in rows:\n",
        "                    term = row.find('div', class_='b-term')\n",
        "                    value = row.find('div', class_='b-value')\n",
        "                    if term and value:\n",
        "                        term_text = term.text.strip().lower()\n",
        "                        if '—Å–ø–µ—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è' in term_text:\n",
        "                            job_data['specialization'] = value.text.strip()\n",
        "                        elif '—É—Ä–æ–≤–µ–Ω—å' in term_text:\n",
        "                            job_data['level'] = value.text.strip()\n",
        "\n",
        "            # –õ–æ–≥–æ—Ç–∏–ø –∫–æ–º–ø–∞–Ω–∏–∏\n",
        "            logo_elem = soup.find('img', {'alt': lambda x: x and 'logo' in x.lower()})\n",
        "            if logo_elem:\n",
        "                job_data['company_logo_url'] = logo_elem.get('src')\n",
        "\n",
        "            # –û–ø–∏—Å–∞–Ω–∏–µ –≤–∞–∫–∞–Ω—Å–∏–∏\n",
        "            description_sections = [\n",
        "                ('b-vacancy-short-description', '–ö—Ä–∞—Ç–∫–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ'),\n",
        "                ('b-vacancy-description', '–ü–æ–ª–Ω–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ')\n",
        "            ]\n",
        "\n",
        "            for class_name, section_name in description_sections:\n",
        "                section = soup.find('section', class_=class_name)\n",
        "                if section:\n",
        "                    for elem in section.stripped_strings:\n",
        "                        if elem.strip():\n",
        "                            job_data['description_text'].append(elem.strip())\n",
        "\n",
        "            # –¢–µ—Ö–Ω–æ–ª–æ–≥–∏–∏/–Ω–∞–≤—ã–∫–∏\n",
        "            stack_container = soup.find('div', class_='b-vacancy-stack-container')\n",
        "            if stack_container:\n",
        "                skills = [skill.text.strip() for skill in stack_container.find_all('span', class_='g-label')]\n",
        "                job_data['skills'] = skills\n",
        "\n",
        "            # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Ç–µ–∫—Å—Ç –æ–ø–∏—Å–∞–Ω–∏—è\n",
        "            job_data['description_text'] = '\\n'.join(job_data['description_text'])\n",
        "\n",
        "            return job_data\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ –æ–ø–∏—Å–∞–Ω–∏—è –≤–∞–∫–∞–Ω—Å–∏–∏ {job_url}: {e}\")\n",
        "            return {'url': job_url, 'error': str(e)}\n",
        "\n",
        "    def get_job_urls(self, page: int = 1) -> List[str]:\n",
        "        \"\"\"–ü–æ–ª—É—á–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ URL –≤–∞–∫–∞–Ω—Å–∏–π —Å —Å—Ç—Ä–∞–Ω–∏—Ü—ã\"\"\"\n",
        "        params = {\"p\": page}\n",
        "        try:\n",
        "            response = requests.get(self.base_url, params=params, headers=self.headers)\n",
        "            response.raise_for_status()\n",
        "\n",
        "            soup = BeautifulSoup(response.text, 'html.parser')\n",
        "            job_cards = soup.find_all('div', class_='b-vacancy-card')\n",
        "\n",
        "            job_urls = []\n",
        "            for card in job_cards:\n",
        "                title_elem = card.find('h3')\n",
        "                if title_elem:\n",
        "                    link_elem = title_elem.find('a')\n",
        "                    if link_elem and link_elem.get('href'):\n",
        "                        job_urls.append('https://getmatch.ru' + link_elem.get('href'))\n",
        "\n",
        "            return job_urls\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ —Å–ø–∏—Å–∫–∞ –≤–∞–∫–∞–Ω—Å–∏–π —Å–æ —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page}: {e}\")\n",
        "            return []\n",
        "\n",
        "    def scrape_to_csv(self, num_pages: int = 5, output_format: str = 'csv'):\n",
        "        \"\"\"–°–±–æ—Ä –æ–ø–∏—Å–∞–Ω–∏–π –≤–∞–∫–∞–Ω—Å–∏–π —Å –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —Å—Ç—Ä–∞–Ω–∏—Ü\"\"\"\n",
        "        all_jobs = []\n",
        "\n",
        "        for page in range(1, num_pages + 1):\n",
        "            self.logger.info(f\"–û–±—Ä–∞–±–æ—Ç–∫–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page}...\")\n",
        "            job_urls = self.get_job_urls(page)\n",
        "\n",
        "            for url in job_urls:\n",
        "                self.logger.info(f\"–ü–æ–ª—É—á–µ–Ω–∏–µ –æ–ø–∏—Å–∞–Ω–∏—è –≤–∞–∫–∞–Ω—Å–∏–∏: {url}\")\n",
        "                job_data = self.get_job_description(url)\n",
        "                if job_data and 'error' not in job_data:\n",
        "                    all_jobs.append(job_data)\n",
        "                time.sleep(1)  # –ó–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏\n",
        "\n",
        "            time.sleep(2)  # –ó–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É —Å—Ç—Ä–∞–Ω–∏—Ü–∞–º–∏\n",
        "\n",
        "        if all_jobs:\n",
        "            timestamp = time.strftime(\"%Y%m%d_%H%M%S\")\n",
        "\n",
        "            if output_format == 'csv':\n",
        "                df = pd.DataFrame(all_jobs)\n",
        "                filename = f'job_descriptions_{timestamp}.csv'\n",
        "                df.to_csv(filename, index=False, encoding='utf-8')\n",
        "                self.logger.info(f\"–°–æ—Ö—Ä–∞–Ω–µ–Ω–æ {len(all_jobs)} –æ–ø–∏—Å–∞–Ω–∏–π –≤–∞–∫–∞–Ω—Å–∏–π –≤ —Ñ–∞–π–ª {filename}\")\n",
        "            elif output_format == 'json':\n",
        "                filename = f'job_descriptions_{timestamp}.json'\n",
        "                pd.DataFrame(all_jobs).to_json(filename, orient='records', force_ascii=False, indent=2)\n",
        "                self.logger.info(f\"–°–æ—Ö—Ä–∞–Ω–µ–Ω–æ {len(all_jobs)} –æ–ø–∏—Å–∞–Ω–∏–π –≤–∞–∫–∞–Ω—Å–∏–π –≤ —Ñ–∞–π–ª {filename}\")\n",
        "        else:\n",
        "            self.logger.error(\"–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–±—Ä–∞—Ç—å –¥–∞–Ω–Ω—ã–µ –æ –≤–∞–∫–∞–Ω—Å–∏—è—Ö\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    scraper = EnhancedJobScraper()\n",
        "    scraper.scrape_to_csv(num_pages=69)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KABpyt1pbqxt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}