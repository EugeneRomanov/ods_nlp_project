{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Get the list of the jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8578"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "\n",
    "# Step 1: Fetch the areas from the API and extract the set of IDs for Russia\n",
    "def get_russian_area_ids():\n",
    "    url = 'https://api.hh.ru/areas'\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        areas = response.json()\n",
    "        russian_area_ids = set()\n",
    "\n",
    "        def extract_ids(area):\n",
    "            if area['id'] == '113':  # Russia ID\n",
    "                for sub_area in area['areas']:\n",
    "                    russian_area_ids.add(sub_area['id'])\n",
    "                    for sub_sub_area in sub_area['areas']:\n",
    "                        russian_area_ids.add(sub_sub_area['id'])\n",
    "            else:\n",
    "                for sub_area in area['areas']:\n",
    "                    extract_ids(sub_area)\n",
    "\n",
    "        for area in areas:\n",
    "            extract_ids(area)\n",
    "\n",
    "        return russian_area_ids\n",
    "    else:\n",
    "        print(f\"Failed to fetch areas. Status code: {response.status_code}\")\n",
    "        return set()\n",
    "\n",
    "russian_area_ids = get_russian_area_ids()\n",
    "len(russian_area_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "import requests\n",
    "import json\n",
    "import csv\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "API_BASE_URL = \"https://api.hh.ru/vacancies\"\n",
    "DATE_FORMAT = \"%Y-%m-%d\"\n",
    "START_DATE = \"2024-10-01\"\n",
    "END_DATE = \"2024-11-25\"\n",
    "# all roles for IT in hh:\n",
    "PROFESSIONAL_ROLES = ['156', '160', '10', '12', '150', '25', '165', '34', '36', '73', '155', '96', '164', '104', '157', '107', '112', '113', '148', '114', '116', '121', '124', '125', '126']\n",
    "PER_PAGE = 100\n",
    "MAX_PAGES = 20\n",
    "\n",
    "def get_vacancies(date_from, date_to, page):\n",
    "    url = f\"{API_BASE_URL}?only_with_salary=true&date_from={date_from}&date_to={date_to}&page={page}&per_page={PER_PAGE}\"\n",
    "    for role in PROFESSIONAL_ROLES:\n",
    "        url += f\"&professional_role={role}\"\n",
    "    # Headers to mimic Firefox on Mac\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:92.0) Gecko/20100101 Firefox/92.0',\n",
    "        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n",
    "        'Accept-Language': 'ru-RU,ru;q=0.9,en-US;q=0.8,en;q=0.7',\n",
    "        'Accept-Encoding': 'gzip, deflate, br',\n",
    "        'Connection': 'keep-alive',\n",
    "        'Upgrade-Insecure-Requests': '1',\n",
    "        'TE': 'Trailers'\n",
    "    }\n",
    "    max_retries = 5\n",
    "    backoff_factor = 1\n",
    "\n",
    "    for _ in range(max_retries):\n",
    "        try:\n",
    "            # Fetch the page content with a timeout of 1 minute\n",
    "            response = requests.get(url, headers=headers, timeout=60)\n",
    "            if response.status_code == 200:\n",
    "                job = response.json()\n",
    "                return job\n",
    "            else:\n",
    "                print(f\"Failed to fetch for page {page} date {date_from}. Status code: {response.status_code}\")\n",
    "                return {'items': []}\n",
    "        except requests.RequestException as e:\n",
    "            print(f\"Request failed: {e}. Retrying in {backoff_factor} seconds...\")\n",
    "            time.sleep(backoff_factor)\n",
    "            backoff_factor *= 2  # Exponential backoff\n",
    "\n",
    "    raise Exception(f\"Failed to fetch page {page} for date {date_from} after {max_retries} attempts\")\n",
    "\n",
    "def get_vacancy_details(vacancy_id):\n",
    "    url = f\"{API_BASE_URL}/{vacancy_id}\"\n",
    "    # Headers to mimic Firefox on Mac\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:92.0) Gecko/20100101 Firefox/92.0',\n",
    "        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n",
    "        'Accept-Language': 'ru-RU,ru;q=0.9,en-US;q=0.8,en;q=0.7',\n",
    "        'Accept-Encoding': 'gzip, deflate, br',\n",
    "        'Connection': 'keep-alive',\n",
    "        'Upgrade-Insecure-Requests': '1',\n",
    "        'TE': 'Trailers'\n",
    "    }\n",
    "\n",
    "    max_retries = 5\n",
    "    backoff_factor = 1\n",
    "\n",
    "    for _ in range(max_retries):\n",
    "        try:\n",
    "            # Fetch the page content with a timeout of 1 minute\n",
    "            response = requests.get(url, headers=headers, timeout=60)\n",
    "            if response.status_code == 200:\n",
    "                job = response.json()\n",
    "                return job\n",
    "            else:\n",
    "                print(f\"Failed to fetch job details. Status code: {response.status_code}\")\n",
    "                return {\n",
    "                    'id': vacancy_id,\n",
    "                    'error': f\"Failed to fetch job details. Status code: {response.status_code}\",\n",
    "                    'description': 'null',\n",
    "                }\n",
    "                # raise Exception(f\"Failed to fetch vacancy details for {vacancy_id}\")\n",
    "        except requests.RequestException as e:\n",
    "            print(f\"Request failed: {e}. Retrying in {backoff_factor} seconds...\")\n",
    "            time.sleep(backoff_factor)\n",
    "            backoff_factor *= 2  # Exponential backoff\n",
    "\n",
    "    raise Exception(f\"Failed to fetch vacancy details for {vacancy_id} after {max_retries} attempts\")\n",
    "\n",
    "def collect_vacancies():\n",
    "    current_date = datetime.strptime(START_DATE, DATE_FORMAT)\n",
    "    end_date = datetime.strptime(END_DATE, DATE_FORMAT)\n",
    "    all_vacancies = []\n",
    "\n",
    "    while current_date <= end_date:\n",
    "        date_from = current_date.strftime(DATE_FORMAT)\n",
    "        date_to = (current_date + timedelta(days=1)).strftime(DATE_FORMAT)\n",
    "        page = 0\n",
    "        print(f\"Collecting vacancies for {date_from}...\")\n",
    "\n",
    "        while True:\n",
    "            vacancies = get_vacancies(date_from, date_to, page)\n",
    "            if not vacancies['items']:\n",
    "                break\n",
    "            all_vacancies.extend(vacancies['items'])\n",
    "            if len(vacancies['items']) < PER_PAGE or page >= MAX_PAGES - 1:\n",
    "                break\n",
    "            page += 1\n",
    "\n",
    "        current_date += timedelta(days=1)\n",
    "\n",
    "    return all_vacancies\n",
    "\n",
    "def collect_vacancy_details(vacancies):\n",
    "    all_details = []\n",
    "    for vacancy in tqdm(vacancies, desc=\"Collecting vacancy details\"):\n",
    "        details = get_vacancy_details(vacancy['id'])\n",
    "        # sleep for a random time between 0.5 and 1 seconds\n",
    "        time.sleep(0.5 + 0.5 * random.random())\n",
    "        all_details.append(details)\n",
    "    return all_details\n",
    "\n",
    "def process_vacancies_in_batches(vacancies, batch_size=100, start_batch=0):\n",
    "    for i in tqdm(range(start_batch * batch_size, len(vacancies), batch_size), desc=\"Processing batches\"):\n",
    "        batch = vacancies[i:i + batch_size]\n",
    "        # batch_vacancies = [{'id': vacancy_id} for vacancy_id in batch]\n",
    "        batch_details = collect_vacancy_details(batch)\n",
    "\n",
    "        # Save the batch details to a JSON file\n",
    "        batch_file_name = f\"../data/hh_18000_24000/hh_vacancies_batch_{i // batch_size}.json\"\n",
    "        with open(batch_file_name, 'w', encoding='utf-8') as f:\n",
    "            json.dump(batch_details, f, ensure_ascii=False, indent=4)\n",
    "        print(f\"Saved batch {i // batch_size} to {batch_file_name}\")\n",
    "\n",
    "def save_to_csv(vacancies, filename):\n",
    "    with open(filename, mode='w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([\"job_title\", \"link\", \"company\", \"technologies\", \"description_text\", \"salary_from\", \"salary_to\", \"currency\", \"locations\", \"url\", \"salary\"])\n",
    "\n",
    "        for vacancy in vacancies:\n",
    "            job_title = vacancy.get(\"name\", \"\")\n",
    "            link = vacancy.get(\"alternate_url\", \"\")\n",
    "            company = vacancy.get(\"employer\", {}).get(\"name\", \"\")\n",
    "            technologies = \", \".join([skill[\"name\"] for skill in vacancy.get(\"key_skills\", [])])\n",
    "            description_text = vacancy.get(\"description\", \"\").replace(\"\\n\", \" \").replace(\"\\r\", \" \")\n",
    "            salary = vacancy.get(\"salary\", {})\n",
    "            salary_from = salary.get(\"from\", \"\")\n",
    "            salary_to = salary.get(\"to\", \"\")\n",
    "            currency = salary.get(\"currency\", \"\")\n",
    "            locations = vacancy.get(\"area\", {}).get(\"name\", \"\")\n",
    "            url = vacancy.get(\"url\", \"\")\n",
    "            salary_info = f\"{salary_from} - {salary_to} {currency}\"\n",
    "\n",
    "            writer.writerow([job_title, link, company, technologies, description_text, salary_from, salary_to, currency, locations, url, salary_info])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34951\n"
     ]
    }
   ],
   "source": [
    "# open all_vacancies.json and find len(all_vacancies) to get the number of vacancies\n",
    "jobs = []\n",
    "with open('hh_all_vacancies_extra_roles.json', 'r') as f:\n",
    "    jobs = json.load(f)\n",
    "\n",
    "print(len(jobs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18676\n"
     ]
    }
   ],
   "source": [
    "# create a set of job ids and check if there are any duplicates\n",
    "job_ids = set()\n",
    "for job in jobs:\n",
    "    job_ids.add(job['id'])\n",
    "\n",
    "print(len(job_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 34951/34951 [00:00<00:00, 81324.13it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "# write a script that removes duplicates from all_vacancies.json\n",
    "# and saves the result to all_vacancies_no_duplicates.json\n",
    "# for each job, check if the id is already fetched\n",
    "# if yes, check the published_at, keep the latest one\n",
    "# datess are strings, so compare them as datetime objects\n",
    "# date format is 2024-10-07T18:23:45+0300\n",
    "\n",
    "jobs_no_duplicates = {}\n",
    "for job in tqdm(jobs):\n",
    "    if job['id'] not in jobs_no_duplicates:\n",
    "        jobs_no_duplicates[job['id']] = job\n",
    "    else:\n",
    "        if datetime.strptime(job['published_at'], \"%Y-%m-%dT%H:%M:%S%z\") > datetime.strptime(jobs_no_duplicates[job['id']]['published_at'], \"%Y-%m-%dT%H:%M:%S%z\"):\n",
    "            jobs_no_duplicates[job['id']] = job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18676\n"
     ]
    }
   ],
   "source": [
    "print(len(jobs_no_duplicates))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the result to all_vacancies_no_duplicates.json\n",
    "with open('hh_all_vacancies_extra_roles_no_duplicates.json', 'w') as f:\n",
    "    json.dump(list(jobs_no_duplicates.values()), f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out the jobs that are in Russia\n",
    "jobs_no_duplicates_ru = [job for job in jobs_no_duplicates.values() if job['area']['id'] in russian_area_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17334"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(jobs_no_duplicates_ru)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '108664089',\n",
       " 'premium': False,\n",
       " 'name': 'Project-менеджер',\n",
       " 'department': None,\n",
       " 'has_test': False,\n",
       " 'response_letter_required': False,\n",
       " 'area': {'id': '15',\n",
       "  'name': 'Астрахань',\n",
       "  'url': 'https://api.hh.ru/areas/15'},\n",
       " 'salary': {'from': 35000, 'to': 50000, 'currency': 'RUR', 'gross': True},\n",
       " 'type': {'id': 'open', 'name': 'Открытая'},\n",
       " 'address': {'city': 'Астрахань',\n",
       "  'street': 'улица Николая Островского',\n",
       "  'building': '148Е',\n",
       "  'lat': 46.33419042214568,\n",
       "  'lng': 48.064644619712105,\n",
       "  'description': None,\n",
       "  'raw': 'Астрахань, улица Николая Островского, 148Е',\n",
       "  'metro': None,\n",
       "  'metro_stations': [],\n",
       "  'id': '16137035'},\n",
       " 'response_url': None,\n",
       " 'sort_point_distance': None,\n",
       " 'published_at': '2024-10-15T11:29:43+0300',\n",
       " 'created_at': '2024-10-15T11:29:43+0300',\n",
       " 'archived': False,\n",
       " 'apply_alternate_url': 'https://hh.ru/applicant/vacancy_response?vacancyId=108664089',\n",
       " 'show_logo_in_search': None,\n",
       " 'insider_interview': None,\n",
       " 'url': 'https://api.hh.ru/vacancies/108664089?host=hh.ru',\n",
       " 'alternate_url': 'https://hh.ru/vacancy/108664089',\n",
       " 'relations': [],\n",
       " 'employer': {'id': '11255747',\n",
       "  'name': 'Астраханский Технопарк СК',\n",
       "  'url': 'https://api.hh.ru/employers/11255747',\n",
       "  'alternate_url': 'https://hh.ru/employer/11255747',\n",
       "  'logo_urls': {'240': 'https://img.hhcdn.ru/employer-logo/6832651.jpeg',\n",
       "   '90': 'https://img.hhcdn.ru/employer-logo/6832650.jpeg',\n",
       "   'original': 'https://img.hhcdn.ru/employer-logo-original/1303097.jpg'},\n",
       "  'vacancies_url': 'https://api.hh.ru/vacancies?employer_id=11255747',\n",
       "  'accredited_it_employer': False,\n",
       "  'trusted': True},\n",
       " 'snippet': {'requirement': 'Знание основ документооборота и офисных программ. Коммуникабельность. Ответственность. Исполнительность. Желание развиваться.',\n",
       "  'responsibility': 'Организация документооборота, оформление и отправка писем и договоров. Коммуникация с партнёрами, подрядчиками и заказчиками. Контроль остатков в офисе, забота о...'},\n",
       " 'contacts': None,\n",
       " 'schedule': {'id': 'fullDay', 'name': 'Полный день'},\n",
       " 'working_days': [],\n",
       " 'working_time_intervals': [],\n",
       " 'working_time_modes': [],\n",
       " 'accept_temporary': False,\n",
       " 'professional_roles': [{'id': '107', 'name': 'Руководитель проектов'}],\n",
       " 'accept_incomplete_resumes': False,\n",
       " 'experience': {'id': 'noExperience', 'name': 'Нет опыта'},\n",
       " 'employment': {'id': 'full', 'name': 'Полная занятость'},\n",
       " 'adv_response_url': None,\n",
       " 'is_adv_vacancy': False,\n",
       " 'adv_context': None}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jobs_no_duplicates_ru[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the result to all_vacancies_no_duplicates_ru.json\n",
    "with open('hh_all_vacancies_extra_roles_no_duplicates_ru.json', 'w') as f:\n",
    "    json.dump(jobs_no_duplicates_ru, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs_no_duplicates_ru_extra_roles = jobs_no_duplicates_ru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('hh_all_vacancies_no_duplicates_ru.json', 'r') as f:\n",
    "    jobs_no_duplicates_ru_base = json.load(f)\n",
    "    # json.dump(jobs_no_duplicates_ru, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14950"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(jobs_no_duplicates_ru_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32284"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jobs_no_duplicates_ru_combined = jobs_no_duplicates_ru_base + jobs_no_duplicates_ru_extra_roles\n",
    "len(jobs_no_duplicates_ru_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32283"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "job_ids = set()\n",
    "for job in jobs_no_duplicates_ru_combined:\n",
    "    job_ids.add(job['id'])\n",
    "\n",
    "len(job_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32284/32284 [00:00<00:00, 345608.11it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "32283"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove duplicates\n",
    "jobs_no_duplicates_ru_combined_no_duplicates = {}\n",
    "for job in tqdm(jobs_no_duplicates_ru_combined):\n",
    "    if job['id'] not in jobs_no_duplicates_ru_combined_no_duplicates:\n",
    "        jobs_no_duplicates_ru_combined_no_duplicates[job['id']] = job\n",
    "    else:\n",
    "        if datetime.strptime(job['published_at'], \"%Y-%m-%dT%H:%M:%S%z\") > datetime.strptime(jobs_no_duplicates_ru_combined_no_duplicates[job['id']]['published_at'], \"%Y-%m-%dT%H:%M:%S%z\"):\n",
    "            jobs_no_duplicates_ru_combined_no_duplicates[job['id']] = job\n",
    "\n",
    "len(jobs_no_duplicates_ru_combined_no_duplicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs_no_duplicates_ru_combined_no_duplicates = [job for job in jobs_no_duplicates_ru_combined_no_duplicates.values()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the result to hh_all_vacancies_no_duplicates_ru_combined.json\n",
    "with open('hh_all_vacancies_no_duplicates_ru_combined.json', 'w') as f:\n",
    "    json.dump(jobs_no_duplicates_ru_combined_no_duplicates, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '108298685',\n",
       " 'premium': False,\n",
       " 'name': 'Senior Python разработчик',\n",
       " 'department': None,\n",
       " 'has_test': False,\n",
       " 'response_letter_required': False,\n",
       " 'area': {'id': '1', 'name': 'Москва', 'url': 'https://api.hh.ru/areas/1'},\n",
       " 'salary': {'from': None, 'to': 370000, 'currency': 'RUR', 'gross': True},\n",
       " 'type': {'id': 'open', 'name': 'Открытая'},\n",
       " 'address': None,\n",
       " 'response_url': None,\n",
       " 'sort_point_distance': None,\n",
       " 'published_at': '2024-10-07T18:23:45+0300',\n",
       " 'created_at': '2024-10-07T18:23:45+0300',\n",
       " 'archived': True,\n",
       " 'apply_alternate_url': 'https://hh.ru/applicant/vacancy_response?vacancyId=108298685',\n",
       " 'show_logo_in_search': None,\n",
       " 'insider_interview': None,\n",
       " 'url': 'https://api.hh.ru/vacancies/108298685?host=hh.ru',\n",
       " 'alternate_url': 'https://hh.ru/vacancy/108298685',\n",
       " 'relations': [],\n",
       " 'employer': {'id': '2639497',\n",
       "  'name': 'АйТиКвик',\n",
       "  'url': 'https://api.hh.ru/employers/2639497',\n",
       "  'alternate_url': 'https://hh.ru/employer/2639497',\n",
       "  'logo_urls': {'original': 'https://img.hhcdn.ru/employer-logo-original/1273332.png',\n",
       "   '90': 'https://img.hhcdn.ru/employer-logo/6713638.png',\n",
       "   '240': 'https://img.hhcdn.ru/employer-logo/6713639.png'},\n",
       "  'vacancies_url': 'https://api.hh.ru/vacancies?employer_id=2639497',\n",
       "  'accredited_it_employer': False,\n",
       "  'trusted': True},\n",
       " 'snippet': {'requirement': None, 'responsibility': None},\n",
       " 'contacts': None,\n",
       " 'schedule': {'id': 'remote', 'name': 'Удаленная работа'},\n",
       " 'working_days': [],\n",
       " 'working_time_intervals': [],\n",
       " 'working_time_modes': [],\n",
       " 'accept_temporary': True,\n",
       " 'professional_roles': [{'id': '96', 'name': 'Программист, разработчик'}],\n",
       " 'accept_incomplete_resumes': False,\n",
       " 'experience': {'id': 'moreThan6', 'name': 'Более 6 лет'},\n",
       " 'employment': {'id': 'full', 'name': 'Полная занятость'},\n",
       " 'adv_response_url': None,\n",
       " 'is_adv_vacancy': False,\n",
       " 'adv_context': None}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('../data/hh_all_vacancies_no_duplicates_ru_combined.json', 'r') as f:\n",
    "    all_vacancies = json.load(f)\n",
    "\n",
    "all_vacancies[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32283"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_vacancies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Get the info for each job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example for processing jobs # 18000 to 24000, change to the slice you need!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting vacancy details: 100%|██████████| 100/100 [02:40<00:00,  1.61s/it]\n",
      "Processing batches:   4%|▍         | 1/25 [02:40<1:04:16, 160.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved batch 35 to ../data/hh_18000_24000/hh_vacancies_batch_35.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting vacancy details: 100%|██████████| 100/100 [01:58<00:00,  1.18s/it]\n",
      "Processing batches:   8%|▊         | 2/25 [04:39<52:02, 135.76s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved batch 36 to ../data/hh_18000_24000/hh_vacancies_batch_36.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting vacancy details: 100%|██████████| 100/100 [01:57<00:00,  1.18s/it]\n",
      "Processing batches:  12%|█▏        | 3/25 [06:36<46:44, 127.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved batch 37 to ../data/hh_18000_24000/hh_vacancies_batch_37.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to fetch job details. Status code: 404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to fetch job details. Status code: 404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting vacancy details: 100%|██████████| 100/100 [02:05<00:00,  1.26s/it]\n",
      "Processing batches:  16%|█▌        | 4/25 [08:42<44:23, 126.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved batch 38 to ../data/hh_18000_24000/hh_vacancies_batch_38.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting vacancy details: 100%|██████████| 100/100 [01:59<00:00,  1.20s/it]\n",
      "Processing batches:  20%|██        | 5/25 [10:42<41:26, 124.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved batch 39 to ../data/hh_18000_24000/hh_vacancies_batch_39.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting vacancy details: 100%|██████████| 100/100 [01:57<00:00,  1.18s/it]\n",
      "Processing batches:  24%|██▍       | 6/25 [12:40<38:39, 122.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved batch 40 to ../data/hh_18000_24000/hh_vacancies_batch_40.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting vacancy details: 100%|██████████| 100/100 [02:02<00:00,  1.23s/it]\n",
      "Processing batches:  28%|██▊       | 7/25 [14:42<36:42, 122.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved batch 41 to ../data/hh_18000_24000/hh_vacancies_batch_41.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting vacancy details: 100%|██████████| 100/100 [02:08<00:00,  1.29s/it]\n",
      "Processing batches:  32%|███▏      | 8/25 [16:51<35:15, 124.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved batch 42 to ../data/hh_18000_24000/hh_vacancies_batch_42.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting vacancy details: 100%|██████████| 100/100 [02:01<00:00,  1.21s/it]\n",
      "Processing batches:  36%|███▌      | 9/25 [18:53<32:56, 123.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved batch 43 to ../data/hh_18000_24000/hh_vacancies_batch_43.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting vacancy details: 100%|██████████| 100/100 [02:27<00:00,  1.47s/it]\n",
      "Processing batches:  40%|████      | 10/25 [21:20<32:43, 130.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved batch 44 to ../data/hh_18000_24000/hh_vacancies_batch_44.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to fetch job details. Status code: 404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting vacancy details: 100%|██████████| 100/100 [02:27<00:00,  1.47s/it]\n",
      "Processing batches:  44%|████▍     | 11/25 [23:48<31:42, 135.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved batch 45 to ../data/hh_18000_24000/hh_vacancies_batch_45.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting vacancy details: 100%|██████████| 100/100 [01:57<00:00,  1.17s/it]\n",
      "Processing batches:  48%|████▊     | 12/25 [25:45<28:12, 130.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved batch 46 to ../data/hh_18000_24000/hh_vacancies_batch_46.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting vacancy details: 100%|██████████| 100/100 [02:07<00:00,  1.27s/it]\n",
      "Processing batches:  52%|█████▏    | 13/25 [27:52<25:52, 129.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved batch 47 to ../data/hh_18000_24000/hh_vacancies_batch_47.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting vacancy details: 100%|██████████| 100/100 [01:58<00:00,  1.18s/it]\n",
      "Processing batches:  56%|█████▌    | 14/25 [29:51<23:07, 126.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved batch 48 to ../data/hh_18000_24000/hh_vacancies_batch_48.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting vacancy details: 100%|██████████| 100/100 [02:01<00:00,  1.21s/it]\n",
      "Processing batches:  60%|██████    | 15/25 [31:52<20:45, 124.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved batch 49 to ../data/hh_18000_24000/hh_vacancies_batch_49.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Request failed: HTTPSConnectionPool(host='api.hh.ru', port=443): Max retries exceeded with url: /vacancies/110960359 (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x116eeb8d0>: Failed to resolve 'api.hh.ru' ([Errno 8] nodename nor servname provided, or not known)\")). Retrying in 1 seconds...\n",
      "Request failed: HTTPSConnectionPool(host='api.hh.ru', port=443): Max retries exceeded with url: /vacancies/110960359 (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x116f08650>: Failed to resolve 'api.hh.ru' ([Errno 8] nodename nor servname provided, or not known)\")). Retrying in 2 seconds...\n",
      "Request failed: HTTPSConnectionPool(host='api.hh.ru', port=443): Max retries exceeded with url: /vacancies/110960359 (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x116f3e210>: Failed to resolve 'api.hh.ru' ([Errno 8] nodename nor servname provided, or not known)\")). Retrying in 4 seconds...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting vacancy details: 100%|██████████| 100/100 [02:19<00:00,  1.39s/it]\n",
      "Processing batches:  64%|██████▍   | 16/25 [34:11<19:20, 128.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved batch 50 to ../data/hh_18000_24000/hh_vacancies_batch_50.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to fetch job details. Status code: 404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting vacancy details: 100%|██████████| 100/100 [02:10<00:00,  1.31s/it]\n",
      "Processing batches:  68%|██████▊   | 17/25 [36:22<17:15, 129.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved batch 51 to ../data/hh_18000_24000/hh_vacancies_batch_51.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting vacancy details: 100%|██████████| 100/100 [02:19<00:00,  1.40s/it]\n",
      "Processing batches:  72%|███████▏  | 18/25 [38:42<15:28, 132.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved batch 52 to ../data/hh_18000_24000/hh_vacancies_batch_52.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Request failed: HTTPSConnectionPool(host='api.hh.ru', port=443): Read timed out. (read timeout=60). Retrying in 1 seconds...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting vacancy details: 100%|██████████| 100/100 [03:27<00:00,  2.08s/it]\n",
      "Processing batches:  76%|███████▌  | 19/25 [42:09<15:31, 155.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved batch 53 to ../data/hh_18000_24000/hh_vacancies_batch_53.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting vacancy details: 100%|██████████| 100/100 [02:29<00:00,  1.50s/it]\n",
      "Processing batches:  80%|████████  | 20/25 [44:39<12:48, 153.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved batch 54 to ../data/hh_18000_24000/hh_vacancies_batch_54.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Request failed: HTTPSConnectionPool(host='api.hh.ru', port=443): Read timed out. (read timeout=60). Retrying in 1 seconds...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting vacancy details: 100%|██████████| 100/100 [03:40<00:00,  2.20s/it]\n",
      "Processing batches:  84%|████████▍ | 21/25 [48:20<11:34, 173.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved batch 55 to ../data/hh_18000_24000/hh_vacancies_batch_55.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting vacancy details: 100%|██████████| 100/100 [02:22<00:00,  1.43s/it]\n",
      "Processing batches:  88%|████████▊ | 22/25 [50:42<08:13, 164.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved batch 56 to ../data/hh_18000_24000/hh_vacancies_batch_56.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting vacancy details: 100%|██████████| 100/100 [02:27<00:00,  1.48s/it]\n",
      "Processing batches:  92%|█████████▏| 23/25 [53:10<05:18, 159.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved batch 57 to ../data/hh_18000_24000/hh_vacancies_batch_57.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting vacancy details: 100%|██████████| 100/100 [02:12<00:00,  1.32s/it]\n",
      "Processing batches:  96%|█████████▌| 24/25 [55:23<02:31, 151.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved batch 58 to ../data/hh_18000_24000/hh_vacancies_batch_58.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Request failed: HTTPSConnectionPool(host='api.hh.ru', port=443): Read timed out. (read timeout=60). Retrying in 1 seconds...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting vacancy details: 100%|██████████| 100/100 [03:28<00:00,  2.09s/it]\n",
      "Processing batches: 100%|██████████| 25/25 [58:51<00:00, 141.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved batch 59 to ../data/hh_18000_24000/hh_vacancies_batch_59.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Collect details for each vacancy\n",
    "all_vacancy_details = process_vacancies_in_batches(all_vacancies[18000:24000], batch_size=100, start_batch=35)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Postprocessing/cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dmitrii.shiriaev/miniconda3/envs/ods-final-project-salary/lib/python3.11/site-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/dmitrii.shiriaev/Code/learning/ods-nlp-2024/final-project/data/hh_24000_-1\n",
      "/Users/dmitrii.shiriaev/Code/learning/ods-nlp-2024/final-project/scraping\n"
     ]
    }
   ],
   "source": [
    "# write a script that reads all the hh_vacancies_batch_*.json files\n",
    "# and combines them into a single list of dictionaries\n",
    "# and saves the result to all_vacancy_details.json\n",
    "\n",
    "import json\n",
    "import os\n",
    "%cd ../data/hh_24000_-1/\n",
    "all_vacancy_details = []\n",
    "for file in os.listdir():\n",
    "    # if file.startswith(\"hh_vacancies_batch_\") and file.endswith(\".json\"):\n",
    "    if (file.startswith(\"hh_vacancies_batch_\") or file.startswith(\"hh_24000_-1\")) and file.endswith(\".json\"):\n",
    "        with open(file, 'r') as f:\n",
    "            all_vacancy_details.extend(json.load(f))\n",
    "\n",
    "%cd ../../scraping/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8283, 8283)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_vacancy_details), len(set([job['id'] for job in all_vacancy_details]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/all_vacancy_details_24000_-1.json', 'w') as f:\n",
    "    json.dump(all_vacancy_details, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '109755906',\n",
       " 'premium': False,\n",
       " 'billing_type': {'id': 'standard', 'name': 'Стандарт'},\n",
       " 'relations': [],\n",
       " 'name': 'Системный администратор',\n",
       " 'insider_interview': None,\n",
       " 'response_letter_required': False,\n",
       " 'area': {'id': '19', 'name': 'Брянск', 'url': 'https://api.hh.ru/areas/19'},\n",
       " 'salary': {'from': 50000, 'to': None, 'currency': 'RUR', 'gross': False},\n",
       " 'type': {'id': 'open', 'name': 'Открытая'},\n",
       " 'address': {'city': 'Брянск',\n",
       "  'street': 'Фосфоритная улица',\n",
       "  'building': '1В',\n",
       "  'lat': 53.259676,\n",
       "  'lng': 34.476164,\n",
       "  'description': None,\n",
       "  'raw': 'Брянск, Фосфоритная улица, 1В',\n",
       "  'metro': None,\n",
       "  'metro_stations': []},\n",
       " 'allow_messages': True,\n",
       " 'experience': {'id': 'between1And3', 'name': 'От 1 года до 3 лет'},\n",
       " 'schedule': {'id': 'fullDay', 'name': 'Полный день'},\n",
       " 'employment': {'id': 'full', 'name': 'Полная занятость'},\n",
       " 'department': None,\n",
       " 'contacts': None,\n",
       " 'description': '<p><strong>Компания «Фермер»</strong> основана в Брянске в 2004 году.</p> <p>Мы считаемся одной из ведущих поставщиков товаров для сада и огорода, дома и дачи.</p> <p>Не так давно ввели новое направление – зоотовары, которое активно развивается.</p> <p><strong>В</strong> связи <strong>с</strong> развитием и <strong>расширением компании, мы увеличиваем штат!</strong></p> <p><strong>Условия:</strong></p> <ul> <li>Работа в офисе.</li> <li>График работы 5/2 с 9:00 до 18:00. Сб.- Вс. – выходные дни.</li> <li>Официальное оформление по ТК.</li> <li>Своевременная выплата заработной платы.</li> <li>Отпуск 2 раза в год.</li> <li>Подарки детям к НГ</li> <li>Корпоративная скидка 10% на наш ассортимент.</li> <li>Приятные подарки к дню рождения;</li> <li>Компенсация занятий спортом ;</li> <li>Интересные задачи;</li> <li>Возможность карьерного роста;</li> </ul> <p><strong>Обязанности:</strong></p> <ul> <li> <p>Установка и настройка новых рабочих мест сотрудников, установка программного обеспечения;</p> </li> <li> <p>Техническая поддержка пользователей;</p> </li> <li> <p>Обеспечение бесперебойной работы IT инфраструктуры (компьютеры, сервер, сеть);</p> </li> <li> <p>Администрирование серверов Windows 2008/2012R2/2016;</p> </li> <li>Администрирование СУБД Microsoft SQL 2012 (Создание резервной копии и восстановление для 1С Бухгалтерии);</li> <li>Сопровождение IP-телефонии</li> <li>Обеспечить развертывание, настройку и администрирование виртуальных машин Microsoft Hyper-V.</li> <li>Обслуживание кассовой техники</li> </ul> <strong>Требования:</strong> <ul> <li>Обслуживание компьютерного парка 80 ПК, 3 сервера</li> <li>Знания и навык работы с системами Windows Server. А именно: Active Directory, DNS, File server, Print server.</li> <li>Понимание работы таких сервисов, как: dhcp, dns, nat, etc.</li> <li>Понимание принципов работы сетевого оборудования и работы сетевых протоколов (TCP, UDP, ICMP и т.д.).</li> <li>Понимание принципов работы СУБД (MS SQL, MySQL).</li> <li>Опыт работы в 1С, с Клиент-Банками (крипто-про).</li> <li>Знание и понимание устройства ПК, периферийных устройств</li> </ul> <p>Если вы активны, трудолюбивы и планируете развиваться то, возможно,<br />МЫ ЖДЕМ ИМЕННО ВАС !</p> <p> </p>',\n",
       " 'branded_description': None,\n",
       " 'vacancy_constructor_template': None,\n",
       " 'key_skills': [{'name': 'Active Directory'},\n",
       "  {'name': 'TCP/IP'},\n",
       "  {'name': 'Администрирование серверов Windows'},\n",
       "  {'name': 'Настройка ПК'},\n",
       "  {'name': 'DHCP'}],\n",
       " 'accept_handicapped': False,\n",
       " 'accept_kids': False,\n",
       " 'archived': False,\n",
       " 'response_url': None,\n",
       " 'specializations': [],\n",
       " 'professional_roles': [{'id': '113', 'name': 'Системный администратор'}],\n",
       " 'code': None,\n",
       " 'hidden': False,\n",
       " 'quick_responses_allowed': False,\n",
       " 'driver_license_types': [],\n",
       " 'accept_incomplete_resumes': False,\n",
       " 'employer': {'id': '2742410',\n",
       "  'name': 'Фермер',\n",
       "  'url': 'https://api.hh.ru/employers/2742410',\n",
       "  'alternate_url': 'https://hh.ru/employer/2742410',\n",
       "  'logo_urls': {'240': 'https://img.hhcdn.ru/employer-logo/7005071.jpeg',\n",
       "   'original': 'https://img.hhcdn.ru/employer-logo-original/1346282.jpg',\n",
       "   '90': 'https://img.hhcdn.ru/employer-logo/7005070.jpeg'},\n",
       "  'vacancies_url': 'https://api.hh.ru/vacancies?employer_id=2742410',\n",
       "  'accredited_it_employer': False,\n",
       "  'trusted': True},\n",
       " 'published_at': '2024-10-31T14:45:21+0300',\n",
       " 'created_at': '2024-10-31T14:45:21+0300',\n",
       " 'initial_created_at': '2024-10-31T14:45:21+0300',\n",
       " 'negotiations_url': None,\n",
       " 'suitable_resumes_url': None,\n",
       " 'apply_alternate_url': 'https://hh.ru/applicant/vacancy_response?vacancyId=109755906',\n",
       " 'has_test': False,\n",
       " 'test': None,\n",
       " 'alternate_url': 'https://hh.ru/vacancy/109755906',\n",
       " 'working_days': [],\n",
       " 'working_time_intervals': [],\n",
       " 'working_time_modes': [],\n",
       " 'accept_temporary': False,\n",
       " 'languages': [],\n",
       " 'approved': True}"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_vacancy_details[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine the batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32283"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1 = \"../data/hh_0_6000/all_vacancy_details_0_6000.json\"\n",
    "f2 = \"../data/hh_6000_12000/all_vacancy_details_6000_12000.json\"\n",
    "f3 = \"../data/hh_12000_18000/all_vacancy_details_12000_18000.json\"\n",
    "f4 = \"../data/hh_18000_24000/all_vacancy_details_18000_24000.json\"\n",
    "f5 = \"../data/hh_24000_-1/all_vacancy_details_24000_-1.json\"\n",
    "# open all_vacancy_details_0_6000.json, all_vacancy_details_6000_12000.json, all_vacancy_details_12000_18000.json\n",
    "# and combine them into a single list of dictionaries\n",
    "\n",
    "all_vacancy_details = []\n",
    "with open(f1, 'r') as f:\n",
    "    all_vacancy_details.extend(json.load(f))\n",
    "\n",
    "with open(f2, 'r') as f:\n",
    "    all_vacancy_details.extend(json.load(f))\n",
    "\n",
    "with open(f3, 'r') as f:\n",
    "    all_vacancy_details.extend(json.load(f))\n",
    "\n",
    "with open(f4, 'r') as f:\n",
    "    all_vacancy_details.extend(json.load(f))\n",
    "\n",
    "with open(f5, 'r') as f:\n",
    "    all_vacancy_details.extend(json.load(f))\n",
    "\n",
    "len(all_vacancy_details)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32283, 32283)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_vacancy_details), len(set([job['id'] for job in all_vacancy_details]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the result to all_vacancy_details_0_18000.json\n",
    "with open(\"../data/all_vacancy_details_combined.json\", 'w') as f:\n",
    "    json.dump(all_vacancy_details, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32283"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_jobs_json = []\n",
    "with open('../data/hh_all_vacancies_no_duplicates_ru_combined.json', 'r') as f:\n",
    "    all_jobs_json = json.load(f)\n",
    "\n",
    "len(all_jobs_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32283\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# get the set intersection of the ids from all_jobs_json and all_vacancy_details\n",
    "job_ids = set([job['id'] for job in all_jobs_json])\n",
    "details_ids = set([job['id'] for job in all_vacancy_details])\n",
    "test_set = set([str(i) for i in range(18000)])\n",
    "\n",
    "intersection = job_ids.intersection(details_ids)\n",
    "print(len(intersection))\n",
    "\n",
    "intersection = test_set.intersection(details_ids)\n",
    "print(len(intersection))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32283\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# sort all_vacancy_details by id to match the order of all_jobs_json\n",
    "# so that the order in all_vacancy_details matches the order in all_jobs_json\n",
    "# in all_jobs_json, the jobs are not sorted by id\n",
    "# use map to get the index of the job in all_jobs_json\n",
    "# then sort all_vacancy_details by the index\n",
    "# then save the result to all_vacancy_details_sorted.json\n",
    "\n",
    "job_id_mapping = {job['id']: job for job in all_vacancy_details}\n",
    "\n",
    "all_vacancy_details_sorted = []\n",
    "\n",
    "for job in all_jobs_json:\n",
    "    all_vacancy_details_sorted.append(job_id_mapping[job['id']])\n",
    "\n",
    "print(len(all_vacancy_details_sorted))\n",
    "count = 0\n",
    "for id1, id2 in zip(all_jobs_json, all_vacancy_details_sorted):\n",
    "    if id1['id'] != id2['id']:\n",
    "        count += 1\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the result to all_vacancy_details_combined_sorted.json\n",
    "\n",
    "with open('../data/all_vacancy_details_combined_sorted.json', 'w') as f:\n",
    "    json.dump(all_vacancy_details_sorted, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "119\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('109868969', None),\n",
       " ('104147603', 'Системный администратор со знанием 1С'),\n",
       " ('110634418', 'Системный администратор'),\n",
       " ('110761403',\n",
       "  'Старший JS-разработчик 3D- Middle Frontend Developer / ASP.NET RESTful API-JS/TS-React-gRPC-Three.js'),\n",
       " ('110833122', 'Руководитель группы 1С'),\n",
       " ('109704302', 'Офис-менеджер в IT компанию(центр)на неполный рабочий день'),\n",
       " ('109758868',\n",
       "  'Junior системный инженер/DevOps инженер/системный администратор'),\n",
       " ('110804828', 'Инженер-программист'),\n",
       " ('108059937', None),\n",
       " ('107346869', 'Backend разработчик (PHP, Laravel - middle, middle+)')]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check if there are mismatch triplets: id, published_at, name\n",
    "\n",
    "job_ids = set()\n",
    "for job in all_jobs_json[:18000]:\n",
    "    job_ids.add((job['id'], job.get('name')))\n",
    "mismatch_triplets = []\n",
    "for job in all_vacancy_details:\n",
    "    if (job['id'], job.get('name')) not in job_ids:\n",
    "        mismatch_triplets.append((job['id'], job.get('name')))\n",
    "print(len(mismatch_triplets))\n",
    "mismatch_triplets[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18000, 18000)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_ids = [job['id'] for job in all_jobs_json[:18000]]\n",
    "len(first_ids), len(set(first_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18000, 18000)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_ids_details = [job['id'] for job in all_vacancy_details]\n",
    "len(first_ids_details), len(set(first_ids_details))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18000"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count = 0\n",
    "for id1, id2 in zip(first_ids, first_ids_details):\n",
    "    if id1 != id2:\n",
    "        count += 1\n",
    "\n",
    "count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18000\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# get the set intersection of the ids from all_jobs_json and all_vacancy_details\n",
    "job_ids = set([job['id'] for job in all_jobs_json[:18000]])\n",
    "details_ids = set([job['id'] for job in all_vacancy_details[:18000]])\n",
    "test_set = set([str(i) for i in range(18000)])\n",
    "\n",
    "intersection = job_ids.intersection(details_ids)\n",
    "print(len(intersection))\n",
    "\n",
    "intersection = test_set.intersection(details_ids)\n",
    "print(len(intersection))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save resulting dataset as csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import csv\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def clean_html_tags(text):\n",
    "    soup = BeautifulSoup(text, 'html.parser')\n",
    "    return soup.get_text(separator=' ')\n",
    "\n",
    "\n",
    "# Function to extract required information from a single vacancy\n",
    "def extract_vacancy_info(vacancy):\n",
    "    title = vacancy.get('name', '')\n",
    "    url = vacancy.get('alternate_url', '')\n",
    "    company = vacancy.get('employer', {}).get('name', '')\n",
    "    skills = ', '.join(skill['name'] for skill in vacancy.get('key_skills', []))\n",
    "    description = vacancy.get('description', '').replace('\\n', ' ').replace('\\r', ' ')\n",
    "    description = clean_html_tags(description)\n",
    "    salary_from = vacancy.get('salary', {}).get('from', '')\n",
    "    salary_to = vacancy.get('salary', {}).get('to', '')\n",
    "    currency = vacancy.get('salary', {}).get('currency', '')\n",
    "    area = vacancy.get('area', {}).get('name', '')\n",
    "    experience = vacancy.get('experience', {}).get('id', '')\n",
    "    if experience == 'noExperience':\n",
    "        experience_from = 0\n",
    "        experience_to = 0\n",
    "    # if there is match to 'from\\d+to\\d+' pattern, extract the values\n",
    "    elif re.match(r'between(\\d+)And(\\d+)', experience):\n",
    "        experience_from, experience_to = re.match(r'between(\\d+)And(\\d+)', experience).groups()\n",
    "    elif re.match(r'moreThan(\\d+)', experience):\n",
    "        experience_from = re.match(r'moreThan(\\d+)', experience).group(1)\n",
    "        experience_to = -1\n",
    "    else:\n",
    "        experience_from = None\n",
    "        experience_to = None\n",
    "    \n",
    "\n",
    "    \n",
    "    return [url, title, area, company, skills, description, salary_from, salary_to, currency, experience_from, experience_to]\n",
    "\n",
    "# Function to process JSON files and save the extracted information to a CSV file\n",
    "def process_json_filee(json_file, output_csv):\n",
    "    with open(output_csv, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        csvwriter = csv.writer(csvfile)\n",
    "        csvwriter.writerow([\n",
    "                            'url', \n",
    "                            'title', \n",
    "                            'area',\n",
    "                            'company', 'skills', 'description', 'salary_from', 'salary_to', 'currency', 'experience_from', 'experience_to'])\n",
    "        \n",
    "        with open(json_file, 'r', encoding='utf-8') as f:\n",
    "            vacancies = json.load(f)\n",
    "            for vacancy in vacancies:\n",
    "                if vacancy.get('salary') is not None:\n",
    "                    csvwriter.writerow(extract_vacancy_info(vacancy))\n",
    "\n",
    "# Example usage\n",
    "json_file = '../data/all_vacancy_details_combined.json'\n",
    "output_csv = 'vacancies_combined_processed.csv'\n",
    "process_json_filee(json_file, output_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32283, 32283)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open('../data/hh_all_vacancies_no_duplicates_ru_combined.json', 'r') as f:\n",
    "    all_vacancies = json.load(f)\n",
    "\n",
    "len(all_vacancies), len(set([job['id'] for job in all_vacancies]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ods-final-project-salary",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
